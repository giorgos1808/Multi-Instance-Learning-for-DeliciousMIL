{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Instance-Learning for DeliciousMIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import pad_sequences\n",
    "from scipy.spatial.distance import directed_hausdorff \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from scipy.spatial.distance import directed_hausdorff \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.getcwd()\n",
    "dataset_dir = my_path + '/DeliciousMIL/Data/'\n",
    "dataset_dir = dataset_dir.replace('\\\\', '/')\n",
    "\n",
    "input_train_data = dataset_dir + 'train-data.dat'\n",
    "input_test_data = dataset_dir + 'test-data.dat'\n",
    "\n",
    "input_train_label = dataset_dir + 'train-label.dat'\n",
    "input_test_label = dataset_dir + 'test-label.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_tags(input_file, output_file):\n",
    "    with open(input_file, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    pattern = r'<[^>]+>'\n",
    "    content = re.sub(pattern, '', content)\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "\n",
    "def replace_multiple_spaces(input_file, output_file):\n",
    "    with open(output_file, 'w') as file2:\n",
    "        with open(input_file, 'r') as file:\n",
    "            for line in file:\n",
    "                content = line\n",
    "                content = ' '.join(content.split())\n",
    "\n",
    "                file2.write(content)\n",
    "                file2.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_X_train = []\n",
    "with open(input_train_data, 'r') as file:\n",
    "    for line in file:\n",
    "        pattern = r'\\n'\n",
    "        line = re.sub(pattern, '', line)\n",
    "        corpus_X_train.append(line)\n",
    "        \n",
    "corpus_X_test = []\n",
    "with open(input_test_data, 'r') as file:\n",
    "    for line in file:\n",
    "        pattern = r'\\n'\n",
    "        line = re.sub(pattern, '', line)\n",
    "        corpus_X_test.append(line)\n",
    "        \n",
    "corpus_y_train = []\n",
    "with open(input_train_label, 'r') as file:\n",
    "    for line in file:\n",
    "        pattern = r'\\n'\n",
    "        line = re.sub(pattern, '', line)\n",
    "        content = line.split(\" \")\n",
    "\n",
    "        for i in range(0, len(content)):\n",
    "            content[i] = int(content[i])\n",
    "\n",
    "        content = np.array(content)\n",
    "\n",
    "        corpus_y_train.append(content)\n",
    "\n",
    "corpus_y_test = []\n",
    "with open(input_test_label, 'r') as file:\n",
    "    for line in file:\n",
    "        pattern = r'\\n'\n",
    "        line = re.sub(pattern, '', line)\n",
    "        content = line.split(\" \")\n",
    "\n",
    "        for i in range(0, len(content)):\n",
    "            content[i] = int(content[i])\n",
    "\n",
    "        corpus_y_test.append(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determine the most frequent class among the 20 classes to create a binary classification problem. Following that, a dataframe is generated containing sets of sentences categorized as either belonging or not belonging to the most frequent class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bags_of_sentences(documents_corpus: list, labels_corpus: list) -> pd.DataFrame:\n",
    "    # Count the occurrences of each label class\n",
    "    labels = np.array(labels_corpus)\n",
    "    class_counts = np.sum(labels, axis=0)\n",
    "\n",
    "    # Find the index of the most frequent label class\n",
    "    most_frequent_class_index = np.argmax(class_counts)\n",
    "\n",
    "    print(\"Most frequent class index:\", most_frequent_class_index)\n",
    "    labels = labels[:, most_frequent_class_index]\n",
    "\n",
    "    # Initialize counters and bag of sentences dictionary\n",
    "    document_counter = 0\n",
    "    sentence_count = 0\n",
    "    bag_of_sentences = {}\n",
    "\n",
    "    for document, label in zip(documents_corpus, labels):\n",
    "        # Split document into sentences\n",
    "        sentences = re.split(r'<\\d+>', document)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Remove leading and trailing whitespaces\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            # If sentence is not empty\n",
    "            if sentence:\n",
    "                # Store words as a list of strings\n",
    "                words = sentence.split(\" \")\n",
    "                # Add a sentence to the bag\n",
    "                bag_of_sentences[sentence_count] = (document_counter, words, label)\n",
    "                sentence_count += 1\n",
    "\n",
    "        document_counter += 1\n",
    "\n",
    "    # Create dataframe of the bag\n",
    "    df = pd.DataFrame.from_dict(bag_of_sentences, orient='index',\n",
    "                                columns=['Bag', 'Sentence', 'Class'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent class index: 2\n",
      "Most frequent class index: 2\n",
      "   Bag                                           Sentence  Class\n",
      "0    0    [6705, 5997, 8310, 3606, 674, 8058, 5044, 4836]      1\n",
      "1    0                           [4312, 5154, 8310, 4225]      1\n",
      "2    1                            [1827, 1037, 8482, 483]      1\n",
      "3    1  [3567, 6172, 6172, 2892, 1362, 787, 399, 777, ...      1\n",
      "4    1      [318, 769, 4621, 3199, 1480, 6213, 971, 6890]      1\n"
     ]
    }
   ],
   "source": [
    "train_df = create_bags_of_sentences(corpus_X_train, corpus_y_train)\n",
    "test_df = create_bags_of_sentences(corpus_X_test, corpus_y_train)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the dataframe into a bag of sentences per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bag_per_document(df: pd.DataFrame):\n",
    "\n",
    "    ids, X, y = np.array(df['Bag']), np.array(df['Sentence']), np.array(df['Class'])\n",
    "    un_id = np.unique(ids)\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(un_id.shape[0]):\n",
    "        bag = X[ids == un_id[i]]\n",
    "        data.append(bag)\n",
    "        label = y[ids == un_id[i]]\n",
    "        labels.append(label)\n",
    "\n",
    "    data = np.array(data, dtype=object)\n",
    "    labels = np.array(labels, dtype=object)\n",
    "    labels = np.array([labels[i][0] for i in range(labels.shape[0])])\n",
    "    \n",
    "    # Pad sentences for 40 words per sentence max.\n",
    "    for i, sentence in enumerate(data):\n",
    "        data[i] = pad_sequences(sentence, maxlen=40)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Get the bags and the labels.\n",
    "train_bag, y_train = create_bag_per_document(train_df)\n",
    "test_bag, y_test = create_bag_per_document(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "We conduct clustering using Hausdorff distance and K-Medoids. Initially, distances for all our data are computed to generate the distance matrix. Subsequently, employing the K-Medoids method, we choose to cluster our data into 3 clusters based on experimentation, which yielded the best silhouette score. Finally, we transform the data and execute the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_hausdorff(x, y):\n",
    "    return max(directed_hausdorff(x, y)[0], directed_hausdorff(y, x)[0])\n",
    "\n",
    "# Initialize distance matrix.\n",
    "n_data = train_bag.shape[0]\n",
    "distance_matrix = np.zeros((n_data, n_data))\n",
    "\n",
    "# Calculate symmetric haussdorff distances.\n",
    "for (i, x), (j, y) in combinations(enumerate(train_bag), 2):\n",
    "    distance_matrix[i, j] = calculate_hausdorff(x, y)\n",
    "    distance_matrix[j, i] = distance_matrix[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def calculate_hausdorff(args):\n",
    "    i, j, x, y = args\n",
    "    return i, j, max(directed_hausdorff(x, y)[0], directed_hausdorff(y, x)[0])\n",
    "\n",
    "def calculate_distance_matrix(args):\n",
    "    i, x, train_bag = args\n",
    "    distances = [calculate_hausdorff((i, j, x, y)) for j, y in enumerate(train_bag)]\n",
    "    return distances\n",
    "\n",
    "# Initialize distance matrix.\n",
    "n_data = train_bag.shape[0]\n",
    "distance_matrix = np.zeros((n_data, n_data))\n",
    "\n",
    "# Use multiprocessing for parallel processing.\n",
    "with Pool() as pool:\n",
    "    args_list = [(i, x, train_bag) for i, x in enumerate(train_bag)]\n",
    "    distance_matrix_list = pool.map(calculate_distance_matrix, args_list)\n",
    "\n",
    "# Fill in the distance matrix.\n",
    "for i, distances in enumerate(distance_matrix_list):\n",
    "    for j, distance in enumerate(distances):\n",
    "        distance_matrix[i, j] = distance[2]\n",
    "        distance_matrix[j, i] = distance[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_medoids_indices = random.sample(range(distance_matrix.shape[0]), 3)\n",
    "init_medoids = train_bag[init_medoids_indices]\n",
    "\n",
    "def k_medoids(distance_matrix, k, max_iter=100):\n",
    "    n_data = distance_matrix.shape[0]\n",
    "    medoid_indices = np.array(init_medoids_indices)\n",
    "    labels = np.zeros(n_data, dtype=int)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        distances = distance_matrix[:, medoid_indices]\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        new_medoid_indices = np.empty(k, dtype=int)\n",
    "        for cluster_label in range(k):\n",
    "            cluster_indices = np.where(labels == cluster_label)[0]\n",
    "            cluster_distances = distances[cluster_indices, cluster_label]\n",
    "            new_medoid_indices[cluster_label] = cluster_indices[np.argmin(cluster_distances)]\n",
    "        \n",
    "        if np.array_equal(medoid_indices, new_medoid_indices):\n",
    "            break\n",
    "        \n",
    "        medoid_indices = new_medoid_indices\n",
    "    \n",
    "    return medoid_indices, labels\n",
    "\n",
    "final_medoids_indices, cluster_labels = k_medoids(distance_matrix, 3)\n",
    "\n",
    "final_medoids = train_bag[final_medoids_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_features(data_bag, medoids) -> np.ndarray:\n",
    "\n",
    "    data_transformed = np.empty((len(data_bag), len(medoids)))\n",
    "\n",
    "    # Generate features, using the distances from the medoids.\n",
    "    for i, x in enumerate(data_bag):\n",
    "        for j, medoid in enumerate(medoids):\n",
    "            data_transformed[i, j] = calculate_hausdorff(x, medoid)\n",
    "\n",
    "    return data_transformed\n",
    "\n",
    "# Transform data.\n",
    "X_train_transformed = generate_features(train_bag, final_medoids)\n",
    "X_test_transformed = generate_features(test_bag, final_medoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'New X train data shape: {X_train_transformed.shape}')\n",
    "print(f'New X test  data shape: {X_test_transformed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=2, max_features=2, random_state=0)\n",
    "models.append([tree, \"Decision Tree\"])\n",
    "svm = LinearSVC(C=0.01)\n",
    "models.append([svm, \"Linear SVM\"])\n",
    "\n",
    "for classifier, name in models:\n",
    "    print(name)\n",
    "    clf = classifier.fit(X_train_transformed, y_train)\n",
    "    y_pred = clf.predict(X_test_transformed)\n",
    "    print(f'Acuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred, average=\"macro\")}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred, average=\"macro\")}')\n",
    "    print(f'F1: {f1_score(y_test, y_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After conducting various experiments with different algorithms, it was observed that the tree-based models exhibited the best performance, albeit still falling short of the desired outcome. The likelihood of substantial information loss is high when considering each sentence as an individual instance in our data. We hypothesize that better results could be achieved by treating each word as an instance and utilizing generalized bags of words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
